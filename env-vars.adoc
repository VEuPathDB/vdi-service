= VDI Stack Environment Configuration
:toc:
:toclevels: 3

== Environment Variables

The following is an index of all the possible environment variables that may be
used to configure the VDI service stack.

=== Modules

==== Rest Service

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

| :exclamation:
| LDAP_SERVER
| List<HostAddress>
|

| :exclamation:
| ORACLE_BASE_DN
| String
|

|===

==== Hard Delete Trigger Handler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| HARD_DELETE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing hard-delete events.

|
| HARD_DELETE_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| HARD_DELETE_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

==== Import Trigger Handler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| IMPORT_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing import events.

|
| IMPORT_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| IMPORT_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
  from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

==== InstallDataTriggerHandler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| INSTALL_DATA_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing install-data events.

|
| INSTALL_DATA_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| INSTALL_DATA_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

==== Share Trigger Handler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| SHARE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing share events.

|
| SHARE_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| SHARE_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

==== Soft Delete Trigger Handler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| SOFT_DELETE_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing soft-delete events.

|
| SOFT_DELETE_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| SOFT_DELETE_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

==== Update Meta Trigger Handler

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

|
| UPDATE_META_HANDLER_WORKER_POOL_SIZE
| uint8
| Number of workers to use while processing update-meta events.

|
| UPDATE_META_HANDLER_WORK_QUEUE_SIZE
| uint16
| Size the worker pool job queue is allowed to fill to before blocking.

| :exclamation:
| UPDATE_META_HANDLER_KAFKA_CONSUMER_CLIENT_ID
| String
| Kafka client ID for the `KafkaConsumer` that will be used to receive messages
from the VDI Kafka instance. +

*THIS VALUE MUST BE UNIQUE ACROSS ALL KAFKA CLIENT IDS*
|===

=== Components

==== Cache DB

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

| :exclamation:
| CACHE_DB_HOST
| String
| Hostname of the cache db instance.

|
| CACHE_DB_PORT
| uint16
| Port number for the cache db instance.

| :exclamation:
| CACHE_DB_NAME
| String
| Name of the postgres database in the cache db instance to use.

| :exclamation:
| CACHE_DB_USERNAME
| String
| Database credentials username.

| :exclamation:
| CACHE_DB_PASSWORD
| String
| Database credentials password.

|
| CACHE_DB_POOL_SIZE
| uint8
| Database connection pool size.
|===

==== Kafka

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

| :exclamation:
| KAFKA_SERVERS
| List<HostAddress>
| Kafka server(s) to connect to publish and consume message topics.
|===

===== Consumer Client

Kafka consumer client tuning and configuration.

[%header, cols="1,3m,3m,8a"]
|===
| Req. | Name | Type | Description

|
| KAFKA_CONSUMER_AUTO_COMMIT_INTERVAL
| Duration
| The frequency that the consumer offsets are auto-committed to Kafka if
  `KAFKA_CONSUMER_ENABLE_AUTO_COMMIT` is set to `true`.

|
| KAFKA_CONSUMER_AUTO_OFFSET_RESET
| "earliest" +
  "latest" +
  "none"
| What to do when there is no initial offset in Kafka, or if the current offset
  does not exist anymore on the server. +

* `earliest` = Automatically reset the offset to the earliest offset.
* `latest` = Automatically reset the offset to the latest offset.
* `none` = Throw an exception if no previous offset is found for the consumer's
  group.

|
| KAFKA_CONSUMER_CONNECTIONS_MAX_IDLE
| Duration
| Close idle connections after this duration.

|
| KAFKA_CONSUMER_DEFAULT_API_TIMEOUT
| Duration
| Specifies the timeout for client APIs.  This configuration is used as the
  default timeout for all client operations that do not specify a `timeout`
  parameter.

|
| KAFKA_CONSUMER_ENABLE_AUTO_COMMIT
| boolean
| If `true`, the consumer's offset will be periodically committed in the
  background.

|
| KAFKA_CONSUMER_FETCH_MAX_BYTES
| uint32
| The maximum amount of data the server should return for a fetch request.
  Records are fetched in batches by the consumer, and if the first record batch
  in the first non-empty partition of the fetch is larger than this value, the
  record batch will still be returned to ensure that the consumer can make
  progress. As such, this is not an absolute maximum.  Note that the consumer
  performs multiple fetches in parallel.

|
| KAFKA_CONSUMER_FETCH_MIN_BYTES
| uint32
| The minimum amount of data the server should return for a fetch request.  If
  insufficient data is available the request will wait for that much data to
  accumulate before answering the request.  The default setting of `1` byte
  means that fetch requests are answered as soon as a single byte of data is
  available or the fetch request times out waiting for data to arrive.  Setting
  this to something greater than `1` will cause the server to wait for larger
  amounts of data to accumulate which can improve server throughput a bit at the
  cost of some additional latency.

| :exclamation:
| KAFKA_CONSUMER_GROUP_ID
| String
| A unique string that identifies the consumer group this consumer belongs to.

|
| KAFKA_CONSUMER_GROUP_INSTANCE_ID
| String
| A unique identifier of the consumer instance provided by the end user.  Only
  non-empty strings are permitted.  If set, the consumer is treated as a static
  member, which means that only one instance with this ID is allowed in the
  consumer group at any time.  This can be used in combination with a larger
  session timeout to avoid group rebalances caused by transient unavailability
  (e.g. process restarts).  If not set, the consumer will join the group as a
  dynamic member, which is the traditional behavior.

|
| KAFKA_CONSUMER_HEARTBEAT_INTERVAL
| Duration
| The expected time between heartbeats to the consumer coordinator when using
  Kafka's group management facilities.  Heartbeats are used to ensure that the
  consumer's session stays active and to facilitate rebalancing when new
  consumers join or leave the group.  The value must be set lower than
  `KAFKA_CONSUMER_SESSION_TIMEOUT`, but typically should be set no higher than
  1/3 of that value.  It can be adjusted even lower to control the expected time
  for normal rebalances.

|
| KAFKA_CONSUMER_MAX_POLL_INTERVAL
| Duration
| The maximum delay between invocations of `poll()` when using consumer group
  management.  This places an upper bound on the amount of time that the
  consumer can be idle before fetching more records.  If `poll()` is not called
  before expiration of this timeout, then the consumer is considered failed and
  the group will rebalance in order to reassign the partitions to another
  member.  For consumers using a non-null `KAFKA_CONSUMER_GROUP_INSTANCE_ID`
  which reach this timeout, partitions will not be immediately reassigned.
  Instead, the consumer will stop sending heartbeats and partitions will be
  reassigned after expiration of `KAFKA_CONSUMER_SESSION_TIMEOUT`.  This mirrors
  the behavior of a static consumer which has shutdown.

|
| KAFKA_CONSUMER_MAX_POLL_RECORDS
| uint32
| The maximum number of records returned in a single call to `poll()`.  Note,
  that this value does not impact the underlying fetching behavior.  The
  consumer will cache the records from each fetch request and returns them
  incrementally from each poll.
|===

=== Wildcard Environment Variables

==== Plugin Handler Environment Key Components.

[source]
----
PLUGIN_HANDLER_<NAME>_NAME
PLUGIN_HANDLER_<NAME>_ADDRESS
PLUGIN_HANDLER_<NAME>_PROJECT_IDS
----

Unlike most of the other environment key values defined here, these values
define components of wildcard environment keys which may be specified with any
arbitrary `<NAME>` value between the defined prefix value and suffix options.

The environment variables set using the prefix and suffixes defined below
must appear in groups that contain the indicated suffixes.  For example, given
the `<NAME>` value `"RNASEQ"` the following two environment variables must be
present:

[source]
----
PLUGIN_HANDLER_RNASEQ_NAME
PLUGIN_HANDLER_RNASEQ_ADDRESS
----

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

| :exclamation:
| PLUGIN_HANDLER_<NAME>_NAME
| String
| Name of the plugin handler.  This will generally be the type name of the
  dataset type that the plugin handles.

| :exclamation:
| PLUGIN_HANDLER_<NAME>_ADDRESS
| HostAddress
| Address and port of the plugin handler service.

|
| PLUGIN_HANDLER_<NAME>_PROJECT_IDS
| List<String>
| List of project IDs for which the plugin is relevant.  If this value is
  omitted or set to a blank value, the plugin will be considered relevant to all
  projects.
|===

==== Application Database Key Components

[source]
----
DB_CONNECTION_NAME_<NAME>
DB_CONNECTION_LDAP_<NAME>
DB_CONNECTION_USER_<NAME>
DB_CONNECTION_PASS_<NAME>
DB_CONNECTION_POOL_SIZE_<NAME>
----

Unlike most of the other environment key values defined here, these values
define components of wildcard environment keys which may be specified with any
arbitrary `<NAME>` value following the defined prefix option.

The environment variables set using the prefixes defined below must appear
in groups that contain all prefixes.  For example, given the `<NAME>` value
`"PLASMO"`, the following environment variables must all be present:

[source]
----
DB_CONNECTION_NAME_PLASMO
DB_CONNECTION_LDAP_PLASMO
DB_CONNECTION_USER_PLASMO
DB_CONNECTION_PASS_PLASMO
DB_CONNECTION_POOL_SIZE_PLASMO
----

[%header, cols="1,3m,3m,8"]
|===
| Req. | Name | Type | Description

| :exclamation:
| DB_CONNECTION_NAME_<NAME>
| String
| Name for the connection, typically the project ID or identifier for the
  application database.

| :exclamation:
| DB_CONNECTION_LDAP_<NAME>
| String
| LDAP distinguished name for the database connection `OrclNetDesc` entry
  containing the connection details for the target database.

| :exclamation:
| DB_CONNECTION_USER_<NAME>
| String
| Database credentials username.

| :exclamation:
| DB_CONNECTION_PASS_<NAME>
| String
| Database credentials password.

| :exclamation:
| DB_CONNECTION_POOL_SIZE_<NAME>
| uint8
| Connection pool size for the JDBC `DataSource`.
|===

== Kafka {

Type: Duration
Required: no
`KAFKA_CONSUMER_POLL_DURATION`::

Type: `UInt`
Required: no
`KAFKA_CONSUMER_RECEIVE_BUFFER_SIZE_BYTES`::

Type: Duration
Required: no
`KAFKA_CONSUMER_RECONNECT_BACKOFF_MAX_TIME`::

Type: Duration
Required: no
`KAFKA_CONSUMER_RECONNECT_BACKOFF_TIME`::

Type: Duration
Required: no
`KAFKA_CONSUMER_REQUEST_TIMEOUT`::

Type: Duration
Required: no
`KAFKA_CONSUMER_RETRY_BACKOFF_TIME`::

Type: `UInt`
Required: no
`KAFKA_CONSUMER_SEND_BUFFER_SIZE_BYTES`::

Type: Duration
Required: no
`KAFKA_CONSUMER_SESSION_TIMEOUT`::
}

object Producer {

Type: `UInt`
Required: no
`KAFKA_PRODUCER_BATCH_SIZE`::

Type: `UInt`
Required: no
`KAFKA_PRODUCER_BUFFER_MEMORY_BYTES`::

Type: `String` +
Required: yes
`KAFKA_PRODUCER_CLIENT_ID`::

Type: Enum("none"|"gzip"|"snappy"|"lz4"|"zstd")
Required: no
`KAFKA_PRODUCER_COMPRESSION_TYPE`::

Type: Duration
Required: no
`KAFKA_PRODUCER_CONNECTIONS_MAX_IDLE`::

Type: Duration
Required: no
`KAFKA_PRODUCER_DELIVERY_TIMEOUT`::

Type: Duration
Required: no
`KAFKA_PRODUCER_LINGER_TIME`::

Type: Duration
Required: no
`KAFKA_PRODUCER_MAX_BLOCKING_TIMEOUT`::

Type: `UInt`
Required: no
`KAFKA_PRODUCER_MAX_REQUEST_SIZE_BYTES`::

Type: Uint
Required: no
`KAFKA_PRODUCER_RECEIVE_BUFFER_SIZE_BYTES`::

Type: Duration
Required: no
`KAFKA_PRODUCER_RECONNECT_BACKOFF_MAX_TIME`::

Type: Duration
Required: no
`KAFKA_PRODUCER_RECONNECT_BACKOFF_TIME`::

Type: Duration
Required: no
`KAFKA_PRODUCER_REQUEST_TIMEOUT`::

Type: Duration
Required: no
`KAFKA_PRODUCER_RETRY_BACKOFF_TIME`::

Type: `UInt`
Required: no
`KAFKA_PRODUCER_SEND_BUFFER_SIZE_BYTES`::

Type: `UInt`
Required: no
`KAFKA_PRODUCER_SEND_RETRIES`::
}

object Topic {

Type: `String` +
Required: no
`KAFKA_TOPIC_HARD_DELETE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_TOPIC_IMPORT_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_TOPIC_INSTALL_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_TOPIC_SHARE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_TOPIC_SOFT_DELETE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_TOPIC_UPDATE_META_TRIGGERS`::
}

object MessageKey {
Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_HARD_DELETE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_IMPORT_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_INSTALL_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_SHARE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_SOFT_DELETE_TRIGGERS`::

Type: `String` +
Required: no
`KAFKA_MESSAGE_KEY_UPDATE_META_TRIGGERS`::
}
}

object Rabbit {
`GLOBAL_RABBIT_CONNECTION_NAME`::
`GLOBAL_RABBIT_HOST`::
`GLOBAL_RABBIT_PASSWORD`::
`GLOBAL_RABBIT_VDI_POLLING_INTERVAL`::
`GLOBAL_RABBIT_PORT`::
`GLOBAL_RABBIT_USERNAME`::

object Exchange {
`GLOBAL_RABBIT_VDI_EXCHANGE_ARGUMENTS`::
`GLOBAL_RABBIT_VDI_EXCHANGE_AUTO_DELETE`::
`GLOBAL_RABBIT_VDI_EXCHANGE_DURABLE`::
`GLOBAL_RABBIT_VDI_EXCHANGE_NAME`::
`GLOBAL_RABBIT_VDI_EXCHANGE_TYPE`::
}

object Queue {
`GLOBAL_RABBIT_VDI_QUEUE_ARGUMENTS`::
`GLOBAL_RABBIT_VDI_QUEUE_AUTO_DELETE`::
`GLOBAL_RABBIT_VDI_QUEUE_EXCLUSIVE`::
`GLOBAL_RABBIT_VDI_QUEUE_DURABLE`::
`GLOBAL_RABBIT_VDI_QUEUE_NAME`::
}

object Routing {
`GLOBAL_RABBIT_VDI_ROUTING_KEY`::
`GLOBAL_RABBIT_VDI_ROUTING_ARGUMENTS`::
}
}

object S3 {

Type: `String` +
Required: yes
`S3_ACCESS_TOKEN`::

Type: `String` +
Required: yes
`S3_BUCKET_NAME`::

Type: `String` +
Required: yes
`S3_HOST`::

Type: UShort
Required: yes
`S3_PORT`::

Type: `String` +
Required: yes
`S3_SECRET_KEY`::

Type: Boolean
Required: yes
`S3_USE_HTTPS`::
}
